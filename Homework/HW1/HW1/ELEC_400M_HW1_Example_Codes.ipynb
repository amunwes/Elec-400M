{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfeBhtmYn1lM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lvKPmnK1d4F"
      },
      "source": [
        "# Q1 (e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Zj5k9X2xec"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWXQ5cEc1Z7v"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Modify the following code and load the training data samples.\n",
        "'''\n",
        "X = np.array([\n",
        "     [0.3, 1.0],\n",
        "     [0.4, 2.0],\n",
        "     [0.5, 3.0]\n",
        "])\n",
        "y = np.array([\n",
        "    3.6,\n",
        "    6.8,\n",
        "    10.0\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XesGqn7z3DwR"
      },
      "outputs": [],
      "source": [
        "# Please use LinearRegression function in sklearn to fit the training set and report the fitted coefficient \n",
        "'''\n",
        "TODO: Write you linear regression code here\n",
        "'''\n",
        "print(\"Theta: \", clf.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDr11b_l4M6q"
      },
      "source": [
        "# Q2 (b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-SmRgeL4aK3"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calculate the cross-entropy loss function given X, y, and theta\n",
        "    \"\"\"\n",
        "    f_x = X.dot(theta)\n",
        "    y_probs = 1 / (1 + np.exp(-f_x))\n",
        "    \n",
        "    '''\n",
        "    TODO: Implement loss calculation here\n",
        "    \n",
        "    loss =\n",
        "\n",
        "    '''\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_grad(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calculate the gradient of the cross-entropy loss w.r.t. theta\n",
        "    \"\"\"\n",
        "    f_x = X.dot(theta)\n",
        "    y_probs = 1 / (1 + np.exp(-f_x))\n",
        "\n",
        "    '''\n",
        "    TODO: Implement loss calculation here\n",
        "    \n",
        "    theta_grad =\n",
        "\n",
        "    '''\n",
        "    return theta_grad\n",
        "\n",
        "\n",
        "def has_converged(loss, new_loss):\n",
        "    \"\"\"\n",
        "    Check if the model has converged and loss remains stable.\n",
        "    \"\"\"\n",
        "    return abs(loss - new_loss) < 0.000001\n",
        "    \n",
        "\n",
        "def calculate_new_theta(theta, theta_grad, alpha):\n",
        "    \"\"\"\n",
        "    Calculate the updated theta based on theta, gradient of theta, and step size\n",
        "    \"\"\"\n",
        "    return theta - alpha * theta_grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RJQwhEk6Vps"
      },
      "outputs": [],
      "source": [
        "# Please load load X and y in training set\n",
        "\n",
        "'''\n",
        "TOTO: Initialize Data. \n",
        "X = \n",
        "y =\n",
        "'''\n",
        "\n",
        "# Initialize parameter and hyperparameters\n",
        "loss = float('inf')\n",
        "new_loss = float('inf')\n",
        "new_theta = np.array([float('inf'), float('inf'), float('inf')])\n",
        "\n",
        "'''\n",
        "TODO: Initlizae theta and alpha\n",
        "theta = \n",
        "alpha = \n",
        "'''\n",
        "\n",
        "iters = 0\n",
        "\n",
        "# Perform Logistic Regression\n",
        "while not has_converged(loss, new_loss):\n",
        "    loss = new_loss\n",
        "    theta_grad = calculate_grad(X, y, theta)\n",
        "    new_theta = calculate_new_theta(theta, theta_grad, alpha)\n",
        "    new_loss = calculate_loss(X, y, new_theta)\n",
        "\n",
        "    theta = new_theta\n",
        "    iters += 1\n",
        "    # Uncomment the following line to print the loss at each step (for debugging purpose)\n",
        "    # print(f\"New Theta: {new_theta}, New Loss: {new_loss}, Iteration {iters}\")\n",
        "\n",
        "print(f\"Final Theta: {theta}, Loss: {calculate_loss(X, y, theta)}, Total Iterations: {iters}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDshGOqGTpWJ"
      },
      "source": [
        "# Q2 (c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNFYT26qsMTH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: fill in theta after one iteration, and theta after convergence.\n",
        "\n",
        "theta_init = [-1.0, -1.5, 0.5]\n",
        "theta_1 = \n",
        "theta_final = \n",
        "'''\n",
        "for theta in [theta_init, theta_1, theta_final]:\n",
        "    '''\n",
        "    TODO: Plot the training data scatter plot, truth labels, and model's decision boundary hyperplane on the same plot.\n",
        "    '''\n",
        "    plt.xlim(0, 4)\n",
        "    plt.ylim(0, 4)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cxmYxq4sMTH"
      },
      "source": [
        "# Q2 (d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyMfZkHI6WJq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCS6-fQ5TvhW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Initialize and load the training data. \n",
        "X = \n",
        "y =\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO12gUJmthxn"
      },
      "outputs": [],
      "source": [
        "# Final Solution\n",
        "\n",
        "'''\n",
        "TODO: Implement logitic regression using function LogisticRegression in sklearn and print the optimized coefficient after convergence. \n",
        "\n",
        "Hint 1: Setting a large max_iter to ensure convergence.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNGthuEsMTH"
      },
      "source": [
        "# Q2 (e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuTLWAtNsMTH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Compare your model with sklearn's model and report accuracy, precision, recall of both models.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OraOV6xvsMTI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}